### Lecture: 9. Decision Trees, Random Forests
#### Date: Nov 28
#### Slides: https://ufal.mff.cuni.cz/~courses/npfl129/2324/slides/?09
#### Reading: https://ufal.mff.cuni.cz/~courses/npfl129/2324/slides.pdf/npfl129-2324-09.pdf,PDF Slides
#### Lecture assignment: decision_tree
#### Lecture assignment: random_forest
#### Lecture assignment: human_activity_recognition
#### Video: https://lectures.ms.mff.cuni.cz/video/rec/npfl129/2324/npfl129-2324-09-czech.mp4, CS Lecture
#### Video: https://lectures.ms.mff.cuni.cz/video/rec/npfl129/2324/npfl129-2324-09-english-part1.mp4, EN Lecture 1/2
#### Video: https://lectures.ms.mff.cuni.cz/video/rec/npfl129/2324/npfl129-2324-09-english-part2.mp4, EN Lecture 2/2
#### Questions: #lecture_9_questions

**Learning objectives.** After the lecture you shoud be able to

- Implement Decision Trees and Random Forests for classification and regression

- Explain how the splitting criterion depend on optimized loss function

- Tell how Random Forests differ from Gradient Boosted Decision Trees

**Covered topics** and where to find more:

- Decision trees [Section 14.4 of PRML]
  - [Decision trees demo](https://mlu-explain.github.io/decision-tree/) by Jared Wilber & Lucía Santamaría

- Random forests
  - [Random forets demo](https://mlu-explain.github.io/random-forest/) by Jenny Yeon & Jared Wilber

After the lecture: short and non-comprehensive [**recap quiz**](http://quest.ms.mff.cuni.cz/class-quiz/quiz/ml_intro_lect09).
